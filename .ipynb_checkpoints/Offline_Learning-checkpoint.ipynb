{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from Grouping import *\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_Caching(N, num_of_video, num_of_client, cache_size, s_len, l_len, K, zipf_param):\n",
    "    def zipf(VN, P, n):\n",
    "        return np.random.choice(VN, n, False, P)\n",
    "\n",
    "    class A2C_Agent:\n",
    "        def __init__(self, state_size, action_size, batch_size):\n",
    "            global advantages\n",
    "            self.state_size = state_size\n",
    "            self.action_size = action_size\n",
    "            self.value_size = 1\n",
    "            self.batch_size = batch_size\n",
    "            advantages = np.zeros((self.batch_size, self.action_size))\n",
    "\n",
    "            self.discount_factor = 0.9\n",
    "            self.actor_lr = 0.001\n",
    "            self.critic_lr = 0.01\n",
    "\n",
    "            self.main_actor = self.build_actor()\n",
    "            self.main_critic = self.build_critic()\n",
    "            self.target_critic = self.build_critic()\n",
    "            self.target_critic.set_weights(self.main_critic.get_weights())\n",
    "\n",
    "        def score_func_loss(self, Y, action_pred):\n",
    "            global advantages\n",
    "            log_lik = -Y * tf.math.log(action_pred)\n",
    "            log_lik_adv = log_lik * advantages\n",
    "            loss = tf.reduce_mean(tf.reduce_sum(log_lik_adv, axis=1))\n",
    "            return loss\n",
    "\n",
    "        def build_actor(self):\n",
    "            actor = tf.keras.models.Sequential()\n",
    "            actor.add(Dense(self.state_size, input_dim=self.state_size, activation='relu',kernel_initializer='he_uniform'))\n",
    "            actor.add(Dense(self.action_size, activation='softmax',kernel_initializer='he_uniform'))\n",
    "            actor.compile(loss=self.score_func_loss, optimizer=tf.keras.optimizers.Adam(lr=self.actor_lr))\n",
    "            return actor\n",
    "\n",
    "        def build_critic(self):\n",
    "            critic = tf.keras.models.Sequential()\n",
    "            critic.add(Dense(self.state_size, input_dim=self.state_size, activation='relu',kernel_initializer='he_uniform'))\n",
    "            critic.add(Dense(self.value_size, activation='linear',kernel_initializer='he_uniform'))\n",
    "            critic.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=self.critic_lr))\n",
    "            return critic\n",
    "\n",
    "        def train_model(self, state_batch, reward_batch, target_train):\n",
    "            global advantages\n",
    "            states = np.vstack([x[0] for x in state_batch])\n",
    "            actions = np.array([x[1] for x in state_batch])\n",
    "            next_states = np.vstack([x[2] for x in state_batch])\n",
    "            rewards = np.vstack([x for x in reward_batch])\n",
    "\n",
    "            target = np.zeros((self.batch_size, self.value_size))\n",
    "            advantages = np.zeros((self.batch_size, self.action_size))\n",
    "\n",
    "            value = self.main_critic.predict(states)\n",
    "            next_value = self.target_critic.predict(next_states)\n",
    "\n",
    "            target = rewards + self.discount_factor * next_value\n",
    "            advantages[range(self.batch_size), actions] = np.reshape(target - value, self.batch_size)\n",
    "\n",
    "            self.main_actor.fit(states, advantages, epochs=1, verbose=0)\n",
    "            self.main_critic.fit(states, target, epochs=1, verbose=0)\n",
    "\n",
    "            if target_train:\n",
    "                self.target_critic.set_weights(self.main_critic.get_weights())\n",
    "\n",
    "    class cache_env:\n",
    "        def __init__(self, VN, cs, s_len, l_len, K, a=1):\n",
    "            self.VN = VN\n",
    "            self.cs = cs\n",
    "            self.K = K\n",
    "            self.s_len = s_len\n",
    "            self.l_len = l_len\n",
    "            self.s_buffer = []\n",
    "            self.l_buffer = []\n",
    "            self.s_cnt = np.zeros(VN)\n",
    "            self.l_cnt = np.zeros(VN)\n",
    "            self.a = a\n",
    "            self.P = np.array([1/(i**self.a) for i in range(1, self.VN+1)])\n",
    "            self.P /= sum(self.P)\n",
    "            np.random.shuffle(self.P)\n",
    "            self.state = []\n",
    "            self.rq = zipf(self.VN, self.P, 1)\n",
    "            self.count()\n",
    "\n",
    "        def step(self, a):\n",
    "            rq = list(self.rq)\n",
    "            prev_state = np.hstack((self.s_cnt[rq + self.state], self.l_cnt[rq + self.state]))\n",
    "            states = None\n",
    "            if a == 'pass':\n",
    "                pass\n",
    "            elif a == 'append':\n",
    "                self.state.append(self.rq[0])\n",
    "            else:\n",
    "                states = [copy.deepcopy(self.state) for _ in range(K)]\n",
    "                for i in range(K):\n",
    "                    if a[i] == 0:\n",
    "                        continue\n",
    "                    states[i].remove(states[i][a[i]-1])\n",
    "                    states[i].append(self.rq[0])\n",
    "            self.rq = zipf(self.VN, self.P, 1)\n",
    "            self.count()\n",
    "            return prev_state, states\n",
    "        \n",
    "        def count(self):\n",
    "            if sum(self.s_cnt) == self.s_len:\n",
    "                self.s_cnt[self.s_buffer[0]] -= 1\n",
    "                self.s_buffer = self.s_buffer[1:]\n",
    "            self.s_cnt[self.rq] += 1\n",
    "            self.s_buffer.append(self.rq[0])\n",
    "            if sum(self.l_cnt) == self.l_len:\n",
    "                self.l_cnt[self.l_buffer[0]] -= 1\n",
    "                self.l_buffer = self.l_buffer[1:]\n",
    "            self.l_cnt[self.rq] += 1\n",
    "            self.l_buffer.append(self.rq[0])\n",
    "            \n",
    "\n",
    "    state_size = 2 * (cache_size + 1)\n",
    "    action_size = cache_size + 1\n",
    "    target_update_fre = 10\n",
    "    \n",
    "    memory_size = 50\n",
    "    state_memory = [deque(maxlen = memory_size) for _ in range(num_of_client)]\n",
    "    reward_memory = [deque(maxlen = memory_size) for _ in range(num_of_client)]\n",
    "    batch_size = 10\n",
    "\n",
    "    reward_list = []\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        Agents = [A2C_Agent(state_size, action_size, batch_size) for _ in range(num_of_client)]\n",
    "        cache = [cache_env(num_of_video, cache_size, s_len, l_len, K, zipf_param) for _ in range(num_of_client)]\n",
    "        for i in range(N):\n",
    "            Train = np.zeros(num_of_client)\n",
    "            for n in range(num_of_client):\n",
    "                if np.random.rand() < ch_p:\n",
    "                    new_P = np.array([1/(i**cache[n].a) for i in range(1, cache[n].VN+1)])\n",
    "                    new_P /= sum(new_P)\n",
    "                    np.random.shuffle(new_P)\n",
    "                    cache[n].P = rho * cache[n].P + (1-rho) * new_P\n",
    "                    \n",
    "                if cache[n].rq in cache[n].state:\n",
    "                    cache[n].step('pass')\n",
    "                elif len(cache[n].state) < cache[n].cs:\n",
    "                    cache[n].step('append')\n",
    "                else:\n",
    "                    Train[n] = 1\n",
    "                    rq = list(cache[n].rq)\n",
    "                    state = np.hstack((cache[n].s_cnt[rq + cache[n].state], cache[n].l_cnt[rq + cache[n].state]))\n",
    "\n",
    "                    pred = Agents[n].main_actor.predict(np.array([state]))[0]\n",
    "                    a_list = np.random.choice(action_size, K, False, p = pred)\n",
    "\n",
    "                    prev_state, states = cache[n].step(a_list)\n",
    "                    rq = list(cache[n].rq)\n",
    "                    state_list = [np.hstack((cache[n].s_cnt[rq + states[i]], cache[n].l_cnt[rq + states[i]])) for i in range(K)]\n",
    "                    critics = Agents[n].main_critic.predict(np.vstack(state_list))\n",
    "                    idx = np.where(critics == max(critics))[0][0]\n",
    "                    a = a_list[idx]\n",
    "                    cache[n].state = states[idx]\n",
    "                    \n",
    "                    state = state_list[idx]\n",
    "                    state_memory[n].append((prev_state, a, state))\n",
    "\n",
    "            rqs = np.array([cache[i].rq for i in range(num_of_client)])\n",
    "            caches = [cache[i].state for i in range(num_of_client)]\n",
    "            cn, rq, ch = Local(rqs, caches)\n",
    "            cn, rq, ch = BCG(list(range(num_of_video)), cn, rq, ch)\n",
    "            cn, rq, ch = XBCG(list(range(num_of_video)), cn, rq, ch)\n",
    "            connection = len(cn)\n",
    "            reward = np.zeros(num_of_client)\n",
    "            local = np.ones(num_of_client)\n",
    "            for c in cn:\n",
    "                if len(c) > 1:\n",
    "                    for i in c:\n",
    "                        reward[i] = 0.5\n",
    "                        local[i] = 0\n",
    "                else:\n",
    "                    local[c[0]] = 0\n",
    "            reward = reward + local\n",
    "            \n",
    "            reward_list.append(sum(reward)/num_of_client)\n",
    "            \n",
    "            for n in range(num_of_client):\n",
    "                if Train[n]:\n",
    "                    reward_memory[n].append(reward[n])\n",
    "                    if len(reward_memory[n]) >= batch_size:\n",
    "                        batch = np.random.choice(len(reward_memory[n]), min(len(reward_memory[n]), batch_size), False)\n",
    "                        state_batch = np.array(state_memory[n])[batch]\n",
    "                        reward_batch = np.array(reward_memory[n])[batch]\n",
    "                        Agents[n].train_model(state_batch, reward_batch, (i+1) % target_update_fre == 0)\n",
    "        \n",
    "        Agents[0].main_actor.save_weights('actor_weights')\n",
    "        Agents[0].main_critic.save_weights('critic_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "num_of_video = 40\n",
    "num_of_client = 10\n",
    "cache_size = 10\n",
    "zipf_param = 1.2\n",
    "s_len = 10\n",
    "l_len = 100\n",
    "K = 5\n",
    "ch_p = 0.001\n",
    "rho = 0.5\n",
    "\n",
    "RL_Caching(N, num_of_video, num_of_client, cache_size, s_len, l_len, K, zipf_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
