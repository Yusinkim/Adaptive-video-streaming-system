{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import cv2\n",
    "import threading\n",
    "import time\n",
    "import struct\n",
    "import pickle\n",
    "import os\n",
    "from functools import reduce\n",
    "import copy\n",
    "\n",
    "class A2C_Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        global advantages\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        self.discount_factor = 0.9\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.01\n",
    "\n",
    "        self.main_actor = self.build_actor()\n",
    "        self.main_critic = self.build_critic()\n",
    "\n",
    "    def build_actor(self):\n",
    "        actor = tf.keras.models.Sequential()\n",
    "        actor.add(Dense(self.state_size, input_dim=self.state_size, activation='relu',kernel_initializer='he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',kernel_initializer='he_uniform'))\n",
    "        return actor\n",
    "\n",
    "    def build_critic(self):\n",
    "        critic = tf.keras.models.Sequential()\n",
    "        critic.add(Dense(self.state_size, input_dim=self.state_size, activation='relu',kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',kernel_initializer='he_uniform'))\n",
    "        return critic\n",
    "    \n",
    "class cache_env:\n",
    "    def __init__(self, VN, V_list, cs, s_len, l_len, K, a=1):\n",
    "        self.VN = VN\n",
    "        self.V_list = V_list\n",
    "        self.cs = cs\n",
    "        self.K = K\n",
    "        self.s_len = s_len\n",
    "        self.l_len = l_len\n",
    "        self.s_buffer = []\n",
    "        self.l_buffer = []\n",
    "        self.s_cnt = np.zeros(VN)\n",
    "        self.l_cnt = np.zeros(VN)\n",
    "        self.a = a\n",
    "        self.P = np.array([1/(i**self.a) for i in range(1, self.VN+1)])\n",
    "        self.P /= sum(self.P)\n",
    "        self.state = os.listdir('Cache2')\n",
    "        self.rq = zipf(self.VN, self.P, 1)\n",
    "        self.count()\n",
    "\n",
    "    def step(self, a):\n",
    "        rq = list(self.rq)\n",
    "        states = None\n",
    "        if a == 'pass':\n",
    "            pass\n",
    "        elif a == 'append':\n",
    "            self.state = os.listdir('Cache2')\n",
    "        else:\n",
    "            states = [copy.deepcopy(self.state) for _ in range(self.K)]\n",
    "            states = [index(self.V_list, states[i]) for i in range(self.K)]\n",
    "            for i in range(self.K):\n",
    "                if a[i] == 0:\n",
    "                    continue\n",
    "                states[i].remove(states[i][a[i]-1])\n",
    "                states[i].append(self.rq[0])\n",
    "        self.rq = zipf(self.VN, self.P, 1)\n",
    "        self.count()\n",
    "        return states\n",
    "        \n",
    "    def count(self):\n",
    "        if sum(self.s_cnt) == self.s_len:\n",
    "            self.s_cnt[self.s_buffer[0]] -= 1\n",
    "            self.s_buffer = self.s_buffer[1:]\n",
    "        self.s_cnt[self.rq] += 1\n",
    "        self.s_buffer.append(self.rq[0])\n",
    "        if sum(self.l_cnt) == self.l_len:\n",
    "            self.l_cnt[self.l_buffer[0]] -= 1\n",
    "            self.l_buffer = self.l_buffer[1:]\n",
    "        self.l_cnt[self.rq] += 1\n",
    "        self.l_buffer.append(self.rq[0])\n",
    "\n",
    "def zipf(VN, P, n):\n",
    "    return np.random.choice(VN, n, False, P)\n",
    "\n",
    "def index(array, elements):\n",
    "    index_list = []\n",
    "    for element in elements:\n",
    "        index_list = index_list + list(np.where(np.array(array) == element)[0])\n",
    "    return index_list\n",
    "\n",
    "def Request(r_v, c_v):\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    ip = '203.255.56.63'\n",
    "    port = 4007\n",
    "    print('Connecting to server...')\n",
    "    sock.connect((ip,port))\n",
    "    num = sock.recv(16).decode()  # 클라이언트 번호 받기\n",
    "    do = 1\n",
    "    if num:\n",
    "        print('Connection successful. You are Client[{}]'.format(num))\n",
    "    else:\n",
    "        print('Connection fail.')\n",
    "        do = 0\n",
    "\n",
    "    sock.send(r_v.encode())\n",
    "    print('Sent r_v({}) to the server!'.format(r_v))\n",
    "    time.sleep(0.001)\n",
    "    sock.send(pickle.dumps(c_v))\n",
    "    print('Sent a caching list{} to the server!'.format(c_v))\n",
    "    print(' ')\n",
    "    return sock\n",
    "    \n",
    "def indexing_mp4(xor_data, v_data, count):\n",
    "    n = len(v_data)\n",
    "    if n == 0:\n",
    "        return xor_data\n",
    "    \n",
    "    datas = []\n",
    "    datas.append(np.frombuffer(xor_data, dtype='uint8'))\n",
    "    xor_len = len(xor_data)\n",
    "    for i in range(n):\n",
    "        datas.append(np.frombuffer(v_data[i][count:count+xor_len], dtype='uint8'))\n",
    "    \n",
    "    def XOR(data1, data2):\n",
    "        l1 = len(data1)\n",
    "        l2 = len(data2)\n",
    "        \n",
    "        if l1 > l2:\n",
    "            xor_len = l2\n",
    "            add_data = data1[l2:l1]\n",
    "        else:\n",
    "            xor_len = l1\n",
    "            add_data = data2[l1:l2]\n",
    "\n",
    "        data = np.bitwise_xor(data1[:xor_len], data2[:xor_len])\n",
    "        data = np.append(data, add_data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    data = reduce(XOR, datas)\n",
    "    data = data.tobytes()\n",
    "    return data\n",
    "\n",
    "def Stream(cache, hit):\n",
    "    global count\n",
    "    r_v = cache.V_list[cache.rq[0]]\n",
    "    c_v = cache.state\n",
    "    \n",
    "    stream_path = 'Cache2/{}'.format(r_v)\n",
    "    if hit:\n",
    "        print('Local Play ({})'.format(r_v))\n",
    "    else:\n",
    "        sock = Request(r_v, c_v)\n",
    "        if sock.recv(16).decode() == 'MC':\n",
    "            MCAST_GRP = sock.recv(16).decode()\n",
    "            MCAST_PORT = int(sock.recv(16).decode())\n",
    "\n",
    "            client = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n",
    "            client.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "            client.bind(('', MCAST_PORT))\n",
    "\n",
    "            mreq = struct.pack(\"4sl\", socket.inet_aton(MCAST_GRP), socket.INADDR_ANY)\n",
    "            client.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)\n",
    "\n",
    "            xor_list = pickle.loads(client.recv(1024))\n",
    "            if len(xor_list) > 1:\n",
    "                print('Receive ({}) by XMC'.format(r_v))\n",
    "                print('XOR list :', xor_list)\n",
    "            else:\n",
    "                print('Receive ({}) by MC'.format(r_v))\n",
    "            xor_list.remove(r_v)\n",
    "            n = len(xor_list)\n",
    "\n",
    "            v_path = []\n",
    "            for i in xor_list:\n",
    "                v_path.append('Cache2/{}'.format(i))\n",
    "\n",
    "            v_data = []\n",
    "            for i in range(n):\n",
    "                with open(v_path[i], 'rb') as v:\n",
    "                    v_data.append(v.read())\n",
    "\n",
    "            n_h = int(client.recv(16).decode())  # 헤더 분열 갯수\n",
    "            n_d = int(client.recv(16).decode())  # 데이터 분열 갯수\n",
    "\n",
    "            header_data = b''\n",
    "            count = 0\n",
    "\n",
    "            for i in range(n_h):  # 헤더 받아서 인덱싱\n",
    "                xor_data = client.recv(65536)\n",
    "                header_data += indexing_mp4(xor_data, v_data, count)\n",
    "                count += len(xor_data)\n",
    "\n",
    "            open(stream_path,'wb').write(header_data)\n",
    "\n",
    "            def recv():  # 데이터 받아서 인덱싱\n",
    "                global count\n",
    "                for i in range(n_d):\n",
    "                    xor_data = client.recv(65536)\n",
    "                    data = indexing_mp4(xor_data, v_data, count)\n",
    "                    if data[0:len(data)] == b'\\x00' * len(data):\n",
    "                        break\n",
    "                    open(stream_path, 'ab').write(data)\n",
    "                    count += len(data)\n",
    "\n",
    "            t = threading.Thread(target = recv)\n",
    "            t.start()\n",
    "        else:\n",
    "            print('Receive ({}) by UC'.format(r_v))\n",
    "            port = int(sock.recv(16).decode())\n",
    "            client = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "            client.bind(('', port))\n",
    "            n_h = int(client.recv(16).decode())  # 헤더 분열 갯수 \n",
    "            n_d = int(client.recv(16).decode())  # 데이터 분열 갯수\n",
    "            \n",
    "            header_data = b''\n",
    "            \n",
    "            for i in range(n_h):\n",
    "                header_data += client.recv(65536)\n",
    "\n",
    "            open(stream_path,'wb').write(header_data)\n",
    "\n",
    "            def recv():\n",
    "                for i in range(n_d):\n",
    "                    data = client.recv(65536)\n",
    "                    open(stream_path, 'ab').write(data)\n",
    "\n",
    "            t = threading.Thread(target = recv)\n",
    "            t.start()\n",
    "\n",
    "    index = 0\n",
    "    cv2.namedWindow(r_v, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    while 1:\n",
    "        video = cv2.VideoCapture(stream_path)\n",
    "\n",
    "        if video.get(5) == 0:  # 받은 데이터가 없다면 잠시 쉬었다가 다시 실행\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        f = round(1/video.get(5)*1000) - 3  # 초당 프레임으로 waitkey값 계산\n",
    "\n",
    "        video.set(1, index)  # 프레임 위치 설정\n",
    "        while 1:\n",
    "            ret, frame = video.read()\n",
    "            if ret:\n",
    "                cv2.imshow(r_v, frame)\n",
    "                cv2.waitKey(f)\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                index = video.get(1)  # 프레임 위치 저장\n",
    "                break\n",
    "\n",
    "        if index < video.get(7):  # 총 프레임수 만큼 재생했다면 종료\n",
    "            time.sleep(1)\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        video.release()\n",
    "\n",
    "    cv2.destroyWindow(r_v)\n",
    "    \n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Connecting to server...\n",
      "Connection successful. You are Client[1]\n",
      "Sent r_v(Beach4.mp4) to the server!\n",
      "Sent a caching list['Beach7.mp4', 'Effect10.mp4', 'Effect9.mp4', 'Light2.mp4', 'Light3.mp4', 'Light4.mp4', 'Light6.mp4', 'Light8.mp4', 'Stars1.mp4', 'Stars10.mp4', 'Stars5.mp4'] to the server!\n",
      " \n",
      "Receive (Beach4.mp4) by UC\n",
      "----------------------------------------\n",
      "['Beach7.mp4', 'Effect10.mp4', 'Effect9.mp4', 'Light2.mp4', 'Light3.mp4', 'Light4.mp4', 'Light6.mp4', 'Light8.mp4', 'Stars1.mp4', 'Stars10.mp4', 'Stars5.mp4']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (22,) but got array with shape (24,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b52ca855fb0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_cnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrq\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcache_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml_cnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrq\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcache_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_actor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0ma_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[1;32m--> 721\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m    722\u001b[0m     return predict_loop(\n\u001b[0;32m    723\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (22,) but got array with shape (24,)"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "\n",
    "V_list = ['Beach{}.mp4'.format(i) for i in range(1,11)] + ['Effect{}.mp4'.format(i) for i in range(1,11)]\n",
    "V_list += ['Light{}.mp4'.format(i) for i in range(1,11)] + ['Stars{}.mp4'.format(i) for i in range(1,11)]\n",
    "num_of_video = len(V_list)\n",
    "cache_size = 10\n",
    "zipf_param = 1.2\n",
    "s_len = 10\n",
    "l_len = 100\n",
    "K = 5\n",
    "ch_p = 0.001\n",
    "\n",
    "state_size = 2 * (cache_size + 1)\n",
    "action_size = cache_size + 1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    Agent = A2C_Agent(state_size, action_size)\n",
    "    Agent.main_actor.load_weights('actor_weights')\n",
    "    Agent.main_critic.load_weights('critic_weights')\n",
    "    cache = cache_env(num_of_video, V_list, cache_size, s_len, l_len, K, zipf_param)\n",
    "    for i in range(N):\n",
    "        if np.random.rand() < ch_p:\n",
    "            new_P = np.array([1/(i**cache.a) for i in range(1, cache.VN+1)])\n",
    "            new_P /= sum(new_P)\n",
    "            np.random.shuffle(new_P)\n",
    "            cache.P = rho * cache.P + (1-rho) * new_P\n",
    "            \n",
    "        if cache.V_list[cache.rq[0]] in cache.state:\n",
    "            Stream(cache, 1)\n",
    "            cache.step('pass')\n",
    "        elif len(cache.state) < cache.cs:\n",
    "            Stream(cache, 0)\n",
    "            cache.step('append')\n",
    "        else:\n",
    "            Stream(cache, 0)\n",
    "            rq = list(cache.rq)\n",
    "            print(cache.state)\n",
    "            cache_index = index(cache.V_list, cache.state) \n",
    "            state = np.hstack((cache.s_cnt[rq + cache_index], cache.l_cnt[rq + cache_index]))\n",
    "\n",
    "            pred = Agent.main_actor.predict(np.array([state]))[0]\n",
    "            a_list = np.random.choice(action_size, K, False, p = pred)\n",
    "\n",
    "            states = cache.step(a_list)\n",
    "            state_list = [np.hstack((cache.s_cnt[rq + states[i]], cache.l_cnt[rq + states[i]])) for i in range(K)]\n",
    "            critics = Agent.main_critic.predict(np.vstack(state_list))\n",
    "            idx = np.where(critics == max(critics))[0][0]\n",
    "\n",
    "            remove_video = list(set(cache.state) - set(states[idx]))[0]\n",
    "            os.remove('Cache2/{}'.format(remove_video))\n",
    "            cache.state = os.listdir('Cache2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
